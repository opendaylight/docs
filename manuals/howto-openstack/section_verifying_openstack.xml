<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE section [
 <!-- Some useful entities borrowed from HTML -->
<!ENTITY ndash  "&#x2013;">
<!ENTITY mdash  "&#x2014;">
<!ENTITY hellip "&#x2026;">
]>
<section xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="verifying_openstack">
    <title>Verifying Openstack is Functioning</title>
    <para>Verify the stack with the following on either host. </para>
    <para>There are two KVM hypervisors registered with Nova. *Note openrc will populate the proper
        Keystone credentials for service client commands. These can be viewed using the export
        command from your shell:</para>
    <para>
        <screen><prompt>[ odl @ fedora - odl - 1 devstack ] $</prompt><command>. . / openrc admin admin</command>
<prompt>[ odl @ fedora - odl - 1 devstack ] $</prompt><command>nova hypervisor - list</command>
<computeroutput>
+----+---------------------+
| ID | Hypervisor hostname |
+----+---------------------+
| 1  | fedora-odl-1        |
| 2  | fedora-odl-2        |
+----+---------------------+</computeroutput></screen>
    </para>
    <para>Note: During the VM Boot Instances, there is a minor configuration differece between
        Fedora 19 and Fedora 20</para>
    <para><emphasis role="bold">Fedora 19</emphasis>:</para>
    <para>
        <screen><command>~/devstack/addimage.sh</command>
<command>export IMAGE=cirros-0.3.0-i386-disk.img</command></screen>
    </para>
    <para><emphasis role="bold">Fedora 20</emphasis>:</para>
    <para>
        <screen><command>export IMAGE = cirros - 0.3.1 - x86_64 - uec</command></screen>
    </para>
    <para>Next, boot a couple of VMs and verify the network overlay is created by
            ODL/OVSDB.</para>
        <para>
            <screen><command>nova boot -- flavor m1 .tiny -- image $ ( nova image - list | grep $IMAGE '\s' | awk '{print $2}' ) -- nic net - id = $ ( neutron net - list | grep private | awk '{print $2}' ) admin - private1</command></screen>
        </para>            
        <para>Boot a 2nd node:</para>
        <para>
            <screen><command>nova boot -- flavor m1 .tiny -- image $ ( nova image - list | grep $IMAGE '\s' | awk '{print $2}' ) -- nic net - id = $ ( neutron net - list | grep private | awk '{print $2}' ) admin - private2</command></screen>
        </para>
        <para>You can also force a host to boot to a particular hypervisor using the following
            (note: this requires an admin role which is implicitly granted to the admin
            user):</para>
        <para>
            <screen><command>nova boot -- flavor m1 .tiny -- image $ ( nova image - list | grep $IMAGE '\s' | awk '{print $2}' ) -- nic net - id = $ ( neutron net - list | grep private | awk '{print $2}' ) demo - private -- availability_zone = nova : fedora - odl - 1</command></screen>
        </para>    
        <para>View the state of the VMs</para>
        <para>
            <screen><prompt>[odl@fedora-odl-1 devstack]$</prompt><userinput>nova list</userinput>
<computeroutput>
+--------------------------------------+----------------+--------+------------+-------------+------------------+
| ID                                   |        Name    | Status | Task State | Power State | Networks         |
+--------------------------------------+----------------+--------+------------+-------------+------------------+
| 01c30219-255a-4376-867a-45d52e349e87 | admin-private1 | ACTIVE | -          | Running     | private=10.0.0.2 |
| bdcfd05b-ebaf-452d-b8c8-81f391a0bb75 | admin-private2 | ACTIVE | -          | Running     | private=10.0.0.4 |
+--------------------------------------+----------------+--------+------------+-------------+------------------+</computeroutput></screen>
        </para>
        <para>To determine where the host is located, look directly at Libvirt using Virsh:</para>
        <para>
            <screen><prompt>[odl@fedora-odl-2 devstack]$</prompt><command>sudo virsh list</command>
<computeroutput>Id Name State
----------------------------------------------------
2 instance-00000002 running</computeroutput></screen>
        </para>
    <para>Ping the endpoints by grabbing a namespace for qdhcp or qrouter. This provides an L3 source
        to ping the VMs. These will only exist on the controller or wherever you are running those
        services in your cloud:</para>
    <para>
        <screen>[odl@fedora-odl-1 devstack]$ ip netns
qdhcp-3f0cfbd2-f23c-481a-8698-3b2dcb7c2657
qrouter-992e450a-875c-4721-9c82-606c283d4f92
[odl@fedora-odl-1 devstack]$ sudo ip netns exec qdhcp-3f0cfbd2-f23c-481a-8698-3b2dcb7c2657 ping 10.0.0.2
PING 10.0.0.2 (10.0.0.2) 56(84) bytes of data.
64 bytes from 10.0.0.2: icmp_seq=1 ttl=64 time=0.737 ms
64 bytes from 10.0.0.2: icmp_seq=2 ttl=64 time=0.578 ms
^C
--- 10.0.0.2 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 0.578/0.657/0.737/0.083 ms
[odl@fedora-odl-1 devstack]$ sudo ip netns exec qdhcp-3f0cfbd2-f23c-481a-8698-3b2dcb7c2657 ping 10.0.0.4
PING 10.0.0.4 (10.0.0.4) 56(84) bytes of data.
64 bytes from 10.0.0.4: icmp_seq=1 ttl=64 time=2.02 ms
64 bytes from 10.0.0.4: icmp_seq=2 ttl=64 time=1.03 ms
^C
--- 10.0.0.4 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 1.037/1.530/2.023/0.493 ms</screen>
    </para>
    <para>Verify the OF13 flow modifications.</para>
    <para>
        <screen>[odl@fedora-odl-2 devstack]$ sudo ovs-ofctl -O OpenFlow13 dump-flows br-int
OFPST_FLOW reply (OF1.3) (xid=0x2):
cookie=0x0, duration=2044.758s, table=0, n_packets=23, n_bytes=2292, send_flow_rem in_port=2,dl_src=fa:16:3e:f5:03:2e actions=set_field:0x1-&amp;gt;tun_id,goto_table:10
cookie=0x0, duration=2051.364s, table=0, n_packets=30, n_bytes=3336, send_flow_rem tun_id=0x1,in_port=1 actions=goto_table:20
cookie=0x0, duration=2049.553s, table=0, n_packets=0, n_bytes=0, send_flow_rem tun_id=0x2,in_port=1 actions=goto_table:20
cookie=0x0, duration=2044.724s, table=0, n_packets=0, n_bytes=0, send_flow_rem priority=8192,in_port=2 actions=drop
cookie=0x0, duration=2576.478s, table=0, n_packets=410, n_bytes=36490, send_flow_rem dl_type=0x88cc actions=CONTROLLER:56
cookie=0x0, duration=2044.578s, table=10, n_packets=0, n_bytes=0, send_flow_rem priority=8192,tun_id=0x1 actions=goto_table:20
cookie=0x0, duration=2051.322s, table=10, n_packets=10, n_bytes=1208, send_flow_rem priority=16384,tun_id=0x1,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=output:1,goto_table:20
cookie=0x0, duration=2049.477s, table=10, n_packets=0, n_bytes=0, send_flow_rem priority=16384,tun_id=0x2,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=output:1,goto_table:20
cookie=0x0, duration=2050.621s, table=10, n_packets=11, n_bytes=944, send_flow_rem tun_id=0x1,dl_dst=fa:16:3e:00:c4:97 actions=output:1,goto_table:20
cookie=0x0, duration=2049.641s, table=10, n_packets=0, n_bytes=0, send_flow_rem tun_id=0x2,dl_dst=fa:16:3e:c6:00:e1 actions=output:1,goto_table:20
cookie=0x0, duration=2051.415s, table=10, n_packets=2, n_bytes=140, send_flow_rem tun_id=0x1,dl_dst=fa:16:3e:f7:3d:96 actions=output:1,goto_table:20
cookie=0x0, duration=2048.058s, table=10, n_packets=0, n_bytes=0, send_flow_rem tun_id=0x1,dl_dst=fa:16:3e:e1:a7:e1 actions=output:1,goto_table:20
cookie=0x0, duration=2044.517s, table=20, n_packets=13, n_bytes=1084, send_flow_rem priority=8192,tun_id=0x1 actions=drop
cookie=0x0, duration=2044.608s, table=20, n_packets=21, n_bytes=2486, send_flow_rem priority=16384,tun_id=0x1,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=output:2
cookie=0x0, duration=2044.666s, table=20, n_packets=17, n_bytes=1898, send_flow_rem tun_id=0x1,dl_dst=fa:16:3e:f5:03:2e actions=output:2</screen>
    </para>
    <para>Define new networks with encaps of VXLAN or GRE along with specifying the segmentation ID.
        In this case GRE:</para>
    <para>
        <screen><command>neutron net-create gre1 --tenant_id $(keystone tenant-list | grep '\sadmin' | awk '{print $2}') --provider:network_type gre --provider:segmentation_id 1300</command>
<command>neutron subnet-create gre1 10.200.1.0/24 --name gre1</command></screen>
    </para>
    <para>
        <screen><command>neutron net-create gre2 --tenant_id $(keystone tenant-list | grep '\sadmin' | awk '{print $2}') --provider:network_type gre --provider:segmentation_id 1310</command>
<command>neutron subnet-create gre2 10.200.2.0/24 --name gre2</command></screen>
    </para>
    <para>And then boot those instances using those networks:</para>
    <para>
        <screen><command>nova boot --flavor m1.tiny --image $(nova image-list | grep $IMAGE'\s' | awk '{print $2}') --nic net-id=$(neutron net-list | grep 'gre1' | awk '{print $2}') gre1-host</command>
<command>nova boot --flavor m1.tiny --image $(nova image-list | grep $IMAGE'\s' | awk '{print $2}') --nic net-id=$(neutron net-list | grep 'gre2' | awk '{print $2}') gre2-host</command></screen>
    </para>
  </section>
