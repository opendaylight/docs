== Group Based Policy OfOverlay User Guide
This section is for Application Developers and Network Administrators
who are looking to use the OfOverlay renderer for Group Based Policy
to create a network virtualization solution on their infrastructure.

=== Overview
The OpenFlow Overlay (OfOverlay) feature enables the OpenFlow Overlay
renderer, which creates a network virtualization solution across nodes
that host Open vSwitch software switches.  The user configures the required
infrastructure, such as instantiating vSwitch bridges, attaching hosts to
the bridges, and creating the tunnel ports on the bridges used in the
OpenFlow Overlay.  The user can then create the policy they want that
allows Endpoints in Endpoint Groups to communicate with each other,
as expressed by the policy, and the OfOverlay renderer creates the
flow-mods on the vSwitches to create the overlay and enforce the
policy.

The nodes participating in the overlay must run Open vSwitch,
as it has features such as Nicira Extensions that are needed in order
to implement the overlay.  Common use cases for the OpenFlow Overlay
renderer include Data Center and Campus.

=== OpenFlow Overlay Architecture
The OpenFlow Overlay feature renders the policy defined by the user
into OpenFlow flow-mods on the devices in the managed infrastructure.
The renderer uses the OpenFlow plugin to communicate with and program
the vSwitches.

The feature uses the OpenFlow plugin to communicate with the vSwitches
and generate the flow-mods to implement the OpenFlow overlay.

Provide information about feature components and how they work together.
Also include information about how the feature integrates with
OpenDaylight. An architecture diagram could help.

=== Configuring OpenFlow Overlay
The OpenFlow Overlay renderer requires additional configuration to
the normal configuration required in Group Based Policy. In addition
to configuring the policy and registering Endpoints, you must also
configure augmentations in both the Inventory model and on the
Endpoints.

Tunnel ports can be created on Open vSwitch using ovs-vsctl commands.
The following is an example of how to create a VXLAN overlay tunnel
port called s1_vxlan0, on bridge s1:

----
sudo ovs-vsctl add-port <bridge name> <port name> -- set interface s1_vxlan0 type=vxlan options:key=flow options:remote_ip=flow
----
To create a VXLAN port named s1_vxlan0 on bridge s1, this would be the command:
----
sudo ovs-vsctl add-port s1 s1_vxlan0 -- set interface s1_vxlan0 type=vxlan options:key=flow options:remote_ip=flow
----

The options key=flow and remote_ip=flow means that the VXLAN tunnel ID
("key") and tunnel destination IP address will be set using flow-mods 
in Open vswitch.

The bridges also must be connected to the controller. This can be
done using another ovs-vsctl command:
----
sudo ovs-vsctl set-controller s1 tcp:<IP address of the controller>:6653
----
To connect bridge s1 to a controller with an IP address of 192.168.194.1,
you would do the following:
----
sudo ovs-vsctl set-controller s1 tcp:192.168.194.1:6653
----

The following is an example of how the Inventory model augmentations
are used to tell Group Based Policy which vSwitches will be managed
by the OpenFlow Overlay renderer. In this case, there are two vSwitches,
openflow:1 and openflow:2, each with an overlay tunnel (openflow:1:1
and openflw:2:1, respectively).

----
PUT http://{{controllerIp}}:8181/restconf/config/opendaylight-inventory:nodes/
{
    "opendaylight-inventory:nodes": {
        "node": [
            {
                "id": "openflow:1", 
                "ofoverlay:tunnel": [
                    {
                        "tunnel-type": "overlay:tunnel-type-vxlan",
                        "ip": "192.168.194.142",
                        "port": 4789,
                        "node-connector-id": "openflow:1:1"
                    }
                ]
            }, 
            {
                "id": "openflow:2", 
                "ofoverlay:tunnel": [
                    {
                        "tunnel-type": "overlay:tunnel-type-vxlan",
                        "ip": "192.168.194.146",
                        "port": 4789,
                        "node-connector-id": "openflow:2:1"
                    }
                ]
            }
        ]
    }
}
----

For instructions on how to configure policy and register Endpoints,
refer to the Group Based Policy Base section of the User Guide.

The following is an example of Endpoint registration using the additional
"port-name" attribute provided by the OpenFlow Overlay augmentation:
----
POST http://{{controllerIp}}:8181/restconf/operations/endpoint:register-endpoint
{
    "input": {
        "endpoint-group": "1eaf9a67-a171-42a8-9282-71cf702f61dd", 
        "network-containment" : "d2779562-ebf1-45e6-93a4-78e2362bc418",
        "l2-context": "7b796915-adf4-4356-b5ca-de005ac410c1", 
        "mac-address": "00:00:00:00:35:02", 
        "l3-address": [
            {
                "ip-address": "10.0.35.2", 
                "l3-context": "cbe0cc07-b8ff-451d-8171-9eef002a8e80"
            }
        ], 
        "port-name": "vethl-h35_2", 
        "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12"
    }
}
----

You should be able to see the location information augmentation
when reading the endpoint back:
----
GET http://{{controllerIp}}:8181/restconf/operational/endpoint:endpoints/endpoint:endpoint/7b796915-adf4-4356-b5ca-de005ac410c1/00:00:00:00:35:02
{
    "endpoint": [
        {
            "l2-context": "7b796915-adf4-4356-b5ca-de005ac410c1",
            "mac-address": "00:00:00:00:35:02",
            "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12",
            "timestamp": 1432216646793,
            "network-containment": "d2779562-ebf1-45e6-93a4-78e2362bc418",
            "ofoverlay:port-name": "vethl-h35_2",
            "ofoverlay:node-connector-id": "openflow:1:2",
            "ofoverlay:node-id": "openflow:1",
            "l3-address": [
                {
                    "l3-context": "cbe0cc07-b8ff-451d-8171-9eef002a8e80",
                    "ip-address": "10.0.35.2"
                }
            ],
            "endpoint-group": "1eaf9a67-a171-42a8-9282-71cf702f61dd"
        }
    ]
}
----
The augmentations show up as fields with the "ofoverlay:" prefix.
The OfOverlay renderer determines this dynamically based on the port
name provided when the Endpoints are registered. In this case, the
Endpoint lives on the bridge "openflow:1" in the OpenDaylight inventory
model, on the port "openflow:1:1", again in the inventory model.

The Inventory model can be used to help debug....


=== Administering or Managing Group Based Policy OpenFlow Overlay
To start the OpenFlow Overlay renderer, do the following:

. Step 1: start karaf
. Step 2: load the OpenFlow Overlay Renderer feature with restconf
----
opendaylight-user@root> feature:install odl-groupbasedpolicy-ofoverlay odl-restconf
----
. Step 3: Verify that the controller is running
After a few seconds, the controller should be up and the OfOverlay renderer
should be running. You can verify this using the following command in
the karaf console:
----
opendaylight-user@root>log:display | grep OFOverlayRenderer
----

You should see something like the following:
----
2015-05-21 09:55:51,712 | INFO  | config-pusher    | OFOverlayRenderer                |
313 - org.opendaylight.groupbasedpolicy.ofoverlay-renderer - 0.2.0.SNAPSHOT | Initialized OFOverlay renderer
----
. Step 4: Configure the vSwitches that will participate in the overlay
. Step 5: Register the Endpoints that will participate in the overlay
. Step 6: Configure any policy

Include related command reference or  operations that you could perform
using the feature. For example viewing network statistics, monitoring
the network,  generating reports, and so on.

=== Tutorials
The groupbasedpolicy repo contains scripts that can be used to
demonstrate the openflow overlay renderer. There is also information
on the Group Based Policy wiki page.

If there is only one tutorial, you skip the "Tutorials" section and
instead just lead with the single tutorial's name.

==== Tutorial: OfOverlay POC with Mininet
The Group-Based Policy implementation for Helium is a Proof of Concept.
This Proof of Concept implementation includes one example of a group-based
policy renderer, based on Open vSwitch and OpenFlow. Users can create policies
and endpoints using the RESTCONF northbound API.

This section will walk you through setting up a simple demo of the OpenFlow
overlay renderer using docker.  This will simulate a scenario with eight VM hosts
connected over a VXLAN tunnel.

===== Overview
An overview of the use case.

===== Prerequisites
Provide any prerequisite information, assumed knowledge, or environment
required to execute the use case.

Start with two running Ubuntu 14.04 systems, which can be either VMs or physical machines.  You'll need a newer version of openvswitch than exists in Ubuntu 14.04, but you only need the user space components so this is easy.  We'll start by installing OVS 2.1.2 or later.

Log into one of your Ubuntu systems, and run:

----
 OVS_VERSION=2.1.2
 sudo apt-get install build-essential fakeroot debhelper libssl-dev
 wget http://openvswitch.org/releases/openvswitch-${OVS_VERSION}.tar.gz
 tar -xzf openvswitch-${OVS_VERSION}.tar.gz
 cd openvswitch-${OVS_VERSION}
 DEB_BUILD_OPTIONS='parallel=8 nocheck' fakeroot debian/rules binary
 cd ..
 sudo dpkg -i openvswitch-common_${OVS_VERSION}-1_amd64.deb openvswitch-switch_${OVS_VERSION}-1_amd64.deb
 sudo apt-get install mininet
----

Now, either run the same commands on the other system, or just copy the openvswitch-common and openvswitch-switch deb files over and install them, plus install mininet from apt.


===== Target Environment
Include any topology requirement for the use case. Ideally, provide
visual (abstract) layout of network diagrams and any other useful visual
aides.


The test script is found in the source tree under +util/testOfOverlay+.  Copy the +.py+ files from this directory to each of your test systems.  Open +config.py+ in an editor.  You can play with this file later, but for now, just find the section that reads:

----
 switches = [{'name': 's1',
              'tunnelIp': '10.160.9.20',
              'dpid': '1'},
             {'name': 's2',
              'tunnelIp': '10.160.9.21',
              'dpid': '2'}]
----

Change the +tunnelIp+ items to be the IP addresses of each of your test systems.  The IP address of host 1 should be assigned to s1 and similarly for host 2 and s2.

===== Instructions
On test host 1, cd to the directory containing the +testOfOverlay+ script and run:

----
 CONTROLLER=10.160.31.238
 sudo ./testOfOverlay.py --local s1 --controller ${CONTROLLER}
----

You'll need to replace the +CONTROLLER+ address with the IP address of the system where you ran your controller.  This will run mininet and set up the hosts that are configured as attached to s1.  When you're finished running this, you'll be at a mininet prompt, but you won't be able to do anything because the policy is not set up.

The output will look like:

----
$ sudo ./testOfOverlay.py --local s1 --controller 10.160.31.238
*** Configuring hosts
h35_2 h35_3 h36_2 h36_3
*** Starting controller
*** Starting 1 switches
s1
POST http://10.160.31.238:8080/restconf/operations/endpoint:register-endpoint
{
    "input": {
        "endpoint-group": "1eaf9a67-a171-42a8-9282-71cf702f61dd",
        "l2-context": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
        "l3-address": [
            {
                "ip-address": "10.0.35.2",
                "l3-context": "f2311f52-890f-4095-8b85-485ec8b92b3c"
            }
        ],
        "mac-address": "00:00:00:00:35:02",
        "ofoverlay:node-connector-id": "openflow:1:1",
        "ofoverlay:node-id": "openflow:1",
        "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12"
    }
}

POST http://10.160.31.238:8080/restconf/operations/endpoint:register-endpoint
{
    "input": {
        "endpoint-group": "1eaf9a67-a171-42a8-9282-71cf702f61dd",
        "l2-context": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
        "l3-address": [
            {
                "ip-address": "10.0.35.3",
                "l3-context": "f2311f52-890f-4095-8b85-485ec8b92b3c"
            }
        ],
        "mac-address": "00:00:00:00:35:03",
        "ofoverlay:node-connector-id": "openflow:1:2",
        "ofoverlay:node-id": "openflow:1",
        "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12"
    }
}

POST http://10.160.31.238:8080/restconf/operations/endpoint:register-endpoint
{
    "input": {
        "endpoint-group": "e593f05d-96be-47ad-acd5-ba81465680d5",
        "l2-context": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
        "l3-address": [
            {
                "ip-address": "10.0.36.2",
                "l3-context": "f2311f52-890f-4095-8b85-485ec8b92b3c"
            }
        ],
        "mac-address": "00:00:00:00:36:02",
        "ofoverlay:node-connector-id": "openflow:1:3",
        "ofoverlay:node-id": "openflow:1",
        "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12"
    }
}

POST http://10.160.31.238:8080/restconf/operations/endpoint:register-endpoint
{
    "input": {
        "endpoint-group": "e593f05d-96be-47ad-acd5-ba81465680d5",
        "l2-context": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
        "l3-address": [
            {
                "ip-address": "10.0.36.3",
                "l3-context": "f2311f52-890f-4095-8b85-485ec8b92b3c"
            }
        ],
        "mac-address": "00:00:00:00:36:03",
        "ofoverlay:node-connector-id": "openflow:1:4",
        "ofoverlay:node-id": "openflow:1",
        "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12"
    }
}

*** Starting CLI:
mininet>
----

On test host 2, you'll do the same but run instead:

----
 CONTROLLER=10.160.31.238
 sudo ./testOfOverlay.py --local s2 --controller ${CONTROLLER} --policy
----

This will run mininet on the other system, and also install all the policy required to enable the connectivity.

The output will look like:

----
$ sudo ./testOfOverlay.py --local s2 --controller ${CONTROLLER} --policy
*** Configuring hosts
h35_4 h35_5 h36_4 h36_5
*** Starting controller
*** Starting 1 switches
s2
PUT http://10.160.31.238:8080/restconf/config/opendaylight-inventory:nodes
{
    "opendaylight-inventory:nodes": {
        "node": [
            {
                "id": "openflow:1",
                "ofoverlay:tunnel-ip": "10.160.9.20"
            },
            {
                "id": "openflow:2",
                "ofoverlay:tunnel-ip": "10.160.9.21"
            }
        ]
    }
}

PUT http://10.160.31.238:8080/restconf/config/policy:tenants
{
    "policy:tenants": {
        "tenant": [
            {
                "contract": [
                    {
                        "clause": [
                            {
                                "name": "allow-http-clause",
                                "subject-refs": [
                                    "allow-http-subject",
                                    "allow-icmp-subject"
                                ]
                            }
                        ],
                        "id": "22282cca-9a13-4d0c-a67e-a933ebb0b0ae",
                        "subject": [
                            {
                                "name": "allow-http-subject",
                                "rule": [
                                    {
                                        "classifier-ref": [
                                            {
                                                "direction": "in",
                                                "name": "http-dest"
                                            },
                                            {
                                                "direction": "out",
                                                "name": "http-src"
                                            }
                                        ],
                                        "name": "allow-http-rule"
                                    }
                                ]
                            },
                            {
                                "name": "allow-icmp-subject",
                                "rule": [
                                    {
                                        "classifier-ref": [
                                            {
                                                "name": "icmp"
                                            }
                                        ],
                                        "name": "allow-icmp-rule"
                                    }
                                ]
                            }
                        ]
                    }
                ],
                "endpoint-group": [
                    {
                        "consumer-named-selector": [
                            {
                                "contract": [
                                    "22282cca-9a13-4d0c-a67e-a933ebb0b0ae"
                                ],
                                "name": "e593f05d-96be-47ad-acd5-ba81465680d5-1eaf9a67-a171-42a8-9282-71cf702f61dd-22282cca-9a13-4d0c-a67e-a933ebb0b0ae"
                            }
                        ],
                        "id": "1eaf9a67-a171-42a8-9282-71cf702f61dd",
                        "network-domain": "77284c12-a569-4585-b244-af9b078acfe4",
                        "provider-named-selector": []
                    },
                    {
                        "consumer-named-selector": [],
                        "id": "e593f05d-96be-47ad-acd5-ba81465680d5",
                        "network-domain": "472ab051-554e-45be-a133-281f0a53412a",
                        "provider-named-selector": [
                            {
                                "contract": [
                                    "22282cca-9a13-4d0c-a67e-a933ebb0b0ae"
                                ],
                                "name": "e593f05d-96be-47ad-acd5-ba81465680d5-1eaf9a67-a171-42a8-9282-71cf702f61dd-22282cca-9a13-4d0c-a67e-a933ebb0b0ae"
                            }
                        ]
                    }
                ],
                "id": "f5c7d344-d1c7-4208-8531-2c2693657e12",
                "l2-bridge-domain": [
                    {
                        "id": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
                        "parent": "f2311f52-890f-4095-8b85-485ec8b92b3c"
                    }
                ],
                "l2-flood-domain": [
                    {
                        "id": "34cc1dd1-2c8c-4e61-a177-588b2d4133b4",
                        "parent": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6"
                    },
                    {
                        "id": "6e669acf-2fd9-48ea-a9b0-cd98d933a6b8",
                        "parent": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6"
                    }
                ],
                "l3-context": [
                    {
                        "id": "f2311f52-890f-4095-8b85-485ec8b92b3c"
                    }
                ],
                "subject-feature-instances": {
                    "classifier-instance": [
                        {
                            "classifier-definition-id": "4250ab32-e8b8-445a-aebb-e1bd2cdd291f",
                            "name": "http-dest",
                            "parameter-value": [
                                {
                                    "name": "type",
                                    "string-value": "TCP"
                                },
                                {
                                    "int-value": "80",
                                    "name": "destport"
                                }
                            ]
                        },
                        {
                            "classifier-definition-id": "4250ab32-e8b8-445a-aebb-e1bd2cdd291f",
                            "name": "http-src",
                            "parameter-value": [
                                {
                                    "name": "type",
                                    "string-value": "TCP"
                                },
                                {
                                    "int-value": "80",
                                    "name": "sourceport"
                                }
                            ]
                        },
                        {
                            "classifier-definition-id": "79c6fdb2-1e1a-4832-af57-c65baf5c2335",
                            "name": "icmp",
                            "parameter-value": [
                                {
                                    "int-value": "1",
                                    "name": "proto"
                                }
                            ]
                        }
                    ]
                },
                "subnet": [
                    {
                        "id": "77284c12-a569-4585-b244-af9b078acfe4",
                        "ip-prefix": "10.0.35.1/24",
                        "parent": "34cc1dd1-2c8c-4e61-a177-588b2d4133b4",
                        "virtual-router-ip": "10.0.35.1"
                    },
                    {
                        "id": "472ab051-554e-45be-a133-281f0a53412a",
                        "ip-prefix": "10.0.36.1/24",
                        "parent": "6e669acf-2fd9-48ea-a9b0-cd98d933a6b8",
                        "virtual-router-ip": "10.0.36.1"
                    }
                ]
            }
        ]
    }
}

POST http://10.160.31.238:8080/restconf/operations/endpoint:register-endpoint
{
    "input": {
        "endpoint-group": "1eaf9a67-a171-42a8-9282-71cf702f61dd",
        "l2-context": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
        "l3-address": [
            {
                "ip-address": "10.0.35.4",
                "l3-context": "f2311f52-890f-4095-8b85-485ec8b92b3c"
            }
        ],
        "mac-address": "00:00:00:00:35:04",
        "ofoverlay:node-connector-id": "openflow:2:1",
        "ofoverlay:node-id": "openflow:2",
        "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12"
    }
}

POST http://10.160.31.238:8080/restconf/operations/endpoint:register-endpoint
{
    "input": {
        "endpoint-group": "1eaf9a67-a171-42a8-9282-71cf702f61dd",
        "l2-context": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
        "l3-address": [
            {
                "ip-address": "10.0.35.5",
                "l3-context": "f2311f52-890f-4095-8b85-485ec8b92b3c"
            }
        ],
        "mac-address": "00:00:00:00:35:05",
        "ofoverlay:node-connector-id": "openflow:2:2",
        "ofoverlay:node-id": "openflow:2",
        "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12"
    }
}

POST http://10.160.31.238:8080/restconf/operations/endpoint:register-endpoint
{
    "input": {
        "endpoint-group": "e593f05d-96be-47ad-acd5-ba81465680d5",
        "l2-context": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
        "l3-address": [
            {
                "ip-address": "10.0.36.4",
                "l3-context": "f2311f52-890f-4095-8b85-485ec8b92b3c"
            }
        ],
        "mac-address": "00:00:00:00:36:04",
        "ofoverlay:node-connector-id": "openflow:2:3",
        "ofoverlay:node-id": "openflow:2",
        "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12"
    }
}

POST http://10.160.31.238:8080/restconf/operations/endpoint:register-endpoint
{
    "input": {
        "endpoint-group": "e593f05d-96be-47ad-acd5-ba81465680d5",
        "l2-context": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
        "l3-address": [
            {
                "ip-address": "10.0.36.5",
                "l3-context": "f2311f52-890f-4095-8b85-485ec8b92b3c"
            }
        ],
        "mac-address": "00:00:00:00:36:05",
        "ofoverlay:node-connector-id": "openflow:2:4",
        "ofoverlay:node-id": "openflow:2",
        "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12"
    }
}

*** Starting CLI:
mininet>
----


In the default test, we have a total of 2 hosts on each switch in each of 2 endpoint groups, for a total of eight hosts.  The endpoints are in two different subnets, so communicating across the two endpoint groups requires routing.  There is a contract set up that allows HTTP from EG1 to EG2, and ICMP in both directions between EG1 and EG2.


We expect ICMP to work between all pairs of hosts.  First, on host one, run pingall as follows:

----
mininet> pingall
*** Ping: testing ping reachability
h35_2 -> h35_3 h36_2 h36_3
h35_3 -> h35_2 h36_2 h36_3
h36_2 -> h35_2 h35_3 h36_3
h36_3 -> h35_2 h35_3 h36_2
*** Results: 0% dropped (12/12 received)
----

and the same on host 2:

----
mininet> pingall
*** Ping: testing ping reachability
h35_4 -> h35_5 h36_4 h36_5
h35_5 -> h35_4 h36_4 h36_5
h36_4 -> h35_4 h35_5 h36_5
h36_5 -> h35_4 h35_5 h36_4
----

The hosts +h35_[n]+ are in EG1, in the subnet 10.0.35.1/24. Hosts +h36_[n]+ are in EG2, in the subnet 10.0.36.1/24.  These two tests therefore shows broadcast within the flood domain working to enable ARP, bridging within the endpoint group, and the functioning of the virtual router which is routing traffic between the two subnets.  It also shows the ICMP policy allowing the ping between the two groups.

Now we can test connectivity over the tunnel:

----
mininet> h35_2 ping -c1 10.0.35.4
PING 10.0.35.4 (10.0.35.4) 56(84) bytes of data.
64 bytes from 10.0.35.4: icmp_seq=1 ttl=64 time=1.78 ms

--- 10.0.35.4 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 1.786/1.786/1.786/0.000 ms
mininet> h35_2 ping -c1 10.0.35.5
PING 10.0.35.5 (10.0.35.5) 56(84) bytes of data.
64 bytes from 10.0.35.5: icmp_seq=1 ttl=64 time=2.59 ms

--- 10.0.35.5 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 2.597/2.597/2.597/0.000 ms
mininet> h35_2 ping -c1 10.0.36.4
PING 10.0.36.4 (10.0.36.4) 56(84) bytes of data.
64 bytes from 10.0.36.4: icmp_seq=1 ttl=62 time=2.64 ms

--- 10.0.36.4 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 2.641/2.641/2.641/0.000 ms
mininet> h35_2 ping -c1 10.0.36.5
PING 10.0.36.5 (10.0.36.5) 56(84) bytes of data.
64 bytes from 10.0.36.5: icmp_seq=1 ttl=62 time=2.93 ms

--- 10.0.36.5 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 2.936/2.936/2.936/0.000 ms
----

This shows all those same features working transparently across the tunnel to the hosts on the other switch.

We expect HTTP to work only when going from EG1 to EG2, and only on port 80.  Let's check.  First, we'll start a web server on +h36_2+ by running this on host 1:

----
 mininet> h36_2 python -m SimpleHTTPServer 80
----

Note that this will block your prompt until you Ctrl-C it later.

Now on host 2, run:

----
mininet> h35_4 curl http://10.0.36.2
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   488  100   488    0     0  72944      0 --:--:-- --:--:-- --:--:-- 97600
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN"><html>
<title>Directory listing for /</title>
<body>
<h2>Directory listing for /</h2>
<hr>
<ul>
<li><a href="config.py">config.py</a>
<li><a href="config.pyc">config.pyc</a>
<li><a href="mininet_gbp.py">mininet_gbp.py</a>
<li><a href="mininet_gbp.pyc">mininet_gbp.pyc</a>
<li><a href="odl_gbp.py">odl_gbp.py</a>
<li><a href="odl_gbp.pyc">odl_gbp.pyc</a>
<li><a href="testOfOverlay.py">testOfOverlay.py</a>
</ul>
<hr>
</body>
</html>
----

You can see that the host in endpoint group 1 is able to access the server in endpoint group 2.

Let's try the reverse.  Ctrl-C the server on host 1 and then run:

----
 mininet> h35_2 python -m SimpleHTTPServer 80
----

We can still access the server from +h35_4+ on host 2, because it's in the same endpoint group:

----
mininet> h35_4 curl http://10.0.35.2
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   488  100   488    0     0  55625      0 --:--:-- --:--:-- --:--:-- 61000
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN"><html>
<title>Directory listing for /</title>
<body>
<h2>Directory listing for /</h2>
<hr>
<ul>
<li><a href="config.py">config.py</a>
<li><a href="config.pyc">config.pyc</a>
<li><a href="mininet_gbp.py">mininet_gbp.py</a>
<li><a href="mininet_gbp.pyc">mininet_gbp.pyc</a>
<li><a href="odl_gbp.py">odl_gbp.py</a>
<li><a href="odl_gbp.pyc">odl_gbp.pyc</a>
<li><a href="testOfOverlay.py">testOfOverlay.py</a>
</ul>
<hr>
</body>
</html>
----

But we cannot access it from +h36_4+ on host 2, because it's in a different endpoint group and our contract allows HTTP only in the other direction:

----
mininet> h36_4 curl http://10.0.35.2 --connect-timeout 3
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0
curl: (28) Connection timed out after 3001 milliseconds
----

=== Contact Information

Mailing List::
groupbasedpolicy-users@lists.opendaylight.org
IRC::
freenode.net #opendaylight-group-policy
Repository::
 https://git.opendaylight.org/gerrit/groupbasedpolicy

