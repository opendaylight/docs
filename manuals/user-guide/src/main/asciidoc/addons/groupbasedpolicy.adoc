== Group-Based Policy

The Group-Based Policy implementation for Helium is a Proof of Concept. This Proof of Concept implementation includes one example of a group-based policy renderer, based on Open vSwitch, OpenFlow, and OVSDB. Users can create policies and endpoints using the RESTCONF northbound API.

=== Architecture and Model

The Group-Based Policy architecture and model are described in the OpenDaylight Developer's Guide.

=== Tutorial

This section will walk you through setting up a simple demo of the OpenFlow overlay renderer using mininet.  This will simulate a scenario with two VM hosts connected over a VXLAN tunnel.

==== Prepare the Environment

Start with two running Ubuntu 14.04 systems, which can be either VMs or physical machines.  You'll need a newer version of openvswitch than exists in Ubuntu 14.04, but you only need the user space components so this is easy.  We'll start by installing OVS 2.1.2 or later.

Log into one of your Ubuntu systems, and run:

----
 OVS_VERSION=2.1.2
 sudo apt-get install build-essential fakeroot debhelper libssl-dev
 wget http://openvswitch.org/releases/openvswitch-${OVS_VERSION}.tar.gz
 tar -xzf openvswitch-${OVS_VERSION}.tar.gz
 cd openvswitch-${OVS_VERSION}
 DEB_BUILD_OPTIONS='parallel=8 nocheck' fakeroot debian/rules binary
 cd ..
 sudo dpkg -i openvswitch-common_${OVS_VERSION}-1_amd64.deb openvswitch-switch_${OVS_VERSION}-1_amd64.deb
 sudo apt-get install mininet
----

Now, either run the same commands on the other system, or just copy the openvswitch-common and openvswitch-switch deb files over and install them, plus install mininet from apt.

==== Configuring the Test

The test script is found in the source tree under +util/testOfOverlay+.  Copy the +.py+ files from this directory to each of your test systems.  Open +config.py+ in an editor.  You can play with this file later, but for now, just find the section that reads:

----
 switches = [{'name': 's1',
              'tunnelIp': '10.160.9.20',
              'dpid': '1'},
             {'name': 's2',
              'tunnelIp': '10.160.9.21',
              'dpid': '2'}]
----

Change the +tunnelIp+ items to be the IP addresses of each of your test systems.  The IP address of host 1 should be assigned to s1 and similarly for host 2 and s2.

==== Running the Test

Now, run the controller.  You can run it on one of your test systems or on a third system.

On test host 1, cd to the directory containing the +testOfOverlay+ script and run:

----
 CONTROLLER=10.160.31.238
 sudo ./testOfOverlay.py --local s1 --controller ${CONTROLLER}
----

You'll need to replace the +CONTROLLER+ address with the IP address of the system where you ran your controller.  This will run mininet and set up the hosts that are configured as attached to s1.  When you're finished running this, you'll be at a mininet prompt, but you won't be able to do anything because the policy is not set up.

The output will look like:

----
$ sudo ./testOfOverlay.py --local s1 --controller 10.160.31.238
*** Configuring hosts
h35_2 h35_3 h36_2 h36_3
*** Starting controller
*** Starting 1 switches
s1
POST http://10.160.31.238:8080/restconf/operations/endpoint:register-endpoint
{
    "input": {
        "endpoint-group": "1eaf9a67-a171-42a8-9282-71cf702f61dd",
        "l2-context": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
        "l3-address": [
            {
                "ip-address": "10.0.35.2",
                "l3-context": "f2311f52-890f-4095-8b85-485ec8b92b3c"
            }
        ],
        "mac-address": "00:00:00:00:35:02",
        "ofoverlay:node-connector-id": "openflow:1:1",
        "ofoverlay:node-id": "openflow:1",
        "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12"
    }
}

POST http://10.160.31.238:8080/restconf/operations/endpoint:register-endpoint
{
    "input": {
        "endpoint-group": "1eaf9a67-a171-42a8-9282-71cf702f61dd",
        "l2-context": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
        "l3-address": [
            {
                "ip-address": "10.0.35.3",
                "l3-context": "f2311f52-890f-4095-8b85-485ec8b92b3c"
            }
        ],
        "mac-address": "00:00:00:00:35:03",
        "ofoverlay:node-connector-id": "openflow:1:2",
        "ofoverlay:node-id": "openflow:1",
        "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12"
    }
}

POST http://10.160.31.238:8080/restconf/operations/endpoint:register-endpoint
{
    "input": {
        "endpoint-group": "e593f05d-96be-47ad-acd5-ba81465680d5",
        "l2-context": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
        "l3-address": [
            {
                "ip-address": "10.0.36.2",
                "l3-context": "f2311f52-890f-4095-8b85-485ec8b92b3c"
            }
        ],
        "mac-address": "00:00:00:00:36:02",
        "ofoverlay:node-connector-id": "openflow:1:3",
        "ofoverlay:node-id": "openflow:1",
        "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12"
    }
}

POST http://10.160.31.238:8080/restconf/operations/endpoint:register-endpoint
{
    "input": {
        "endpoint-group": "e593f05d-96be-47ad-acd5-ba81465680d5",
        "l2-context": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
        "l3-address": [
            {
                "ip-address": "10.0.36.3",
                "l3-context": "f2311f52-890f-4095-8b85-485ec8b92b3c"
            }
        ],
        "mac-address": "00:00:00:00:36:03",
        "ofoverlay:node-connector-id": "openflow:1:4",
        "ofoverlay:node-id": "openflow:1",
        "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12"
    }
}

*** Starting CLI:
mininet>
----

On test host 2, you'll do the same but run instead:

----
 CONTROLLER=10.160.31.238
 sudo ./testOfOverlay.py --local s2 --controller ${CONTROLLER} --policy
----

This will run mininet on the other system, and also install all the policy required to enable the connectivity.

The output will look like:

----
$ sudo ./testOfOverlay.py --local s2 --controller ${CONTROLLER} --policy
*** Configuring hosts
h35_4 h35_5 h36_4 h36_5
*** Starting controller
*** Starting 1 switches
s2
PUT http://10.160.31.238:8080/restconf/config/opendaylight-inventory:nodes
{
    "opendaylight-inventory:nodes": {
        "node": [
            {
                "id": "openflow:1",
                "ofoverlay:tunnel-ip": "10.160.9.20"
            },
            {
                "id": "openflow:2",
                "ofoverlay:tunnel-ip": "10.160.9.21"
            }
        ]
    }
}

PUT http://10.160.31.238:8080/restconf/config/policy:tenants
{
    "policy:tenants": {
        "tenant": [
            {
                "contract": [
                    {
                        "clause": [
                            {
                                "name": "allow-http-clause",
                                "subject-refs": [
                                    "allow-http-subject",
                                    "allow-icmp-subject"
                                ]
                            }
                        ],
                        "id": "22282cca-9a13-4d0c-a67e-a933ebb0b0ae",
                        "subject": [
                            {
                                "name": "allow-http-subject",
                                "rule": [
                                    {
                                        "classifier-ref": [
                                            {
                                                "direction": "in",
                                                "name": "http-dest"
                                            },
                                            {
                                                "direction": "out",
                                                "name": "http-src"
                                            }
                                        ],
                                        "name": "allow-http-rule"
                                    }
                                ]
                            },
                            {
                                "name": "allow-icmp-subject",
                                "rule": [
                                    {
                                        "classifier-ref": [
                                            {
                                                "name": "icmp"
                                            }
                                        ],
                                        "name": "allow-icmp-rule"
                                    }
                                ]
                            }
                        ]
                    }
                ],
                "endpoint-group": [
                    {
                        "consumer-named-selector": [
                            {
                                "contract": [
                                    "22282cca-9a13-4d0c-a67e-a933ebb0b0ae"
                                ],
                                "name": "e593f05d-96be-47ad-acd5-ba81465680d5-1eaf9a67-a171-42a8-9282-71cf702f61dd-22282cca-9a13-4d0c-a67e-a933ebb0b0ae"
                            }
                        ],
                        "id": "1eaf9a67-a171-42a8-9282-71cf702f61dd",
                        "network-domain": "77284c12-a569-4585-b244-af9b078acfe4",
                        "provider-named-selector": []
                    },
                    {
                        "consumer-named-selector": [],
                        "id": "e593f05d-96be-47ad-acd5-ba81465680d5",
                        "network-domain": "472ab051-554e-45be-a133-281f0a53412a",
                        "provider-named-selector": [
                            {
                                "contract": [
                                    "22282cca-9a13-4d0c-a67e-a933ebb0b0ae"
                                ],
                                "name": "e593f05d-96be-47ad-acd5-ba81465680d5-1eaf9a67-a171-42a8-9282-71cf702f61dd-22282cca-9a13-4d0c-a67e-a933ebb0b0ae"
                            }
                        ]
                    }
                ],
                "id": "f5c7d344-d1c7-4208-8531-2c2693657e12",
                "l2-bridge-domain": [
                    {
                        "id": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
                        "parent": "f2311f52-890f-4095-8b85-485ec8b92b3c"
                    }
                ],
                "l2-flood-domain": [
                    {
                        "id": "34cc1dd1-2c8c-4e61-a177-588b2d4133b4",
                        "parent": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6"
                    },
                    {
                        "id": "6e669acf-2fd9-48ea-a9b0-cd98d933a6b8",
                        "parent": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6"
                    }
                ],
                "l3-context": [
                    {
                        "id": "f2311f52-890f-4095-8b85-485ec8b92b3c"
                    }
                ],
                "subject-feature-instances": {
                    "classifier-instance": [
                        {
                            "classifier-definition-id": "4250ab32-e8b8-445a-aebb-e1bd2cdd291f",
                            "name": "http-dest",
                            "parameter-value": [
                                {
                                    "name": "type",
                                    "string-value": "TCP"
                                },
                                {
                                    "int-value": "80",
                                    "name": "destport"
                                }
                            ]
                        },
                        {
                            "classifier-definition-id": "4250ab32-e8b8-445a-aebb-e1bd2cdd291f",
                            "name": "http-src",
                            "parameter-value": [
                                {
                                    "name": "type",
                                    "string-value": "TCP"
                                },
                                {
                                    "int-value": "80",
                                    "name": "sourceport"
                                }
                            ]
                        },
                        {
                            "classifier-definition-id": "79c6fdb2-1e1a-4832-af57-c65baf5c2335",
                            "name": "icmp",
                            "parameter-value": [
                                {
                                    "int-value": "1",
                                    "name": "proto"
                                }
                            ]
                        }
                    ]
                },
                "subnet": [
                    {
                        "id": "77284c12-a569-4585-b244-af9b078acfe4",
                        "ip-prefix": "10.0.35.1/24",
                        "parent": "34cc1dd1-2c8c-4e61-a177-588b2d4133b4",
                        "virtual-router-ip": "10.0.35.1"
                    },
                    {
                        "id": "472ab051-554e-45be-a133-281f0a53412a",
                        "ip-prefix": "10.0.36.1/24",
                        "parent": "6e669acf-2fd9-48ea-a9b0-cd98d933a6b8",
                        "virtual-router-ip": "10.0.36.1"
                    }
                ]
            }
        ]
    }
}

POST http://10.160.31.238:8080/restconf/operations/endpoint:register-endpoint
{
    "input": {
        "endpoint-group": "1eaf9a67-a171-42a8-9282-71cf702f61dd",
        "l2-context": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
        "l3-address": [
            {
                "ip-address": "10.0.35.4",
                "l3-context": "f2311f52-890f-4095-8b85-485ec8b92b3c"
            }
        ],
        "mac-address": "00:00:00:00:35:04",
        "ofoverlay:node-connector-id": "openflow:2:1",
        "ofoverlay:node-id": "openflow:2",
        "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12"
    }
}

POST http://10.160.31.238:8080/restconf/operations/endpoint:register-endpoint
{
    "input": {
        "endpoint-group": "1eaf9a67-a171-42a8-9282-71cf702f61dd",
        "l2-context": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
        "l3-address": [
            {
                "ip-address": "10.0.35.5",
                "l3-context": "f2311f52-890f-4095-8b85-485ec8b92b3c"
            }
        ],
        "mac-address": "00:00:00:00:35:05",
        "ofoverlay:node-connector-id": "openflow:2:2",
        "ofoverlay:node-id": "openflow:2",
        "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12"
    }
}

POST http://10.160.31.238:8080/restconf/operations/endpoint:register-endpoint
{
    "input": {
        "endpoint-group": "e593f05d-96be-47ad-acd5-ba81465680d5",
        "l2-context": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
        "l3-address": [
            {
                "ip-address": "10.0.36.4",
                "l3-context": "f2311f52-890f-4095-8b85-485ec8b92b3c"
            }
        ],
        "mac-address": "00:00:00:00:36:04",
        "ofoverlay:node-connector-id": "openflow:2:3",
        "ofoverlay:node-id": "openflow:2",
        "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12"
    }
}

POST http://10.160.31.238:8080/restconf/operations/endpoint:register-endpoint
{
    "input": {
        "endpoint-group": "e593f05d-96be-47ad-acd5-ba81465680d5",
        "l2-context": "70aeb9ea-4ca1-4fb9-9780-22b04b84a0d6",
        "l3-address": [
            {
                "ip-address": "10.0.36.5",
                "l3-context": "f2311f52-890f-4095-8b85-485ec8b92b3c"
            }
        ],
        "mac-address": "00:00:00:00:36:05",
        "ofoverlay:node-connector-id": "openflow:2:4",
        "ofoverlay:node-id": "openflow:2",
        "tenant": "f5c7d344-d1c7-4208-8531-2c2693657e12"
    }
}

*** Starting CLI:
mininet>
----

==== Verifying

In the default test, we have a total of 2 hosts on each switch in each of 2 endpoint groups, for a total of eight hosts.  The endpoints are in two different subnets, so communicating across the two endpoint groups requires routing.  There is a contract set up that allows HTTP from EG1 to EG2, and ICMP in both directions between EG1 and EG2.

===== ICMP

We expect ICMP to work between all pairs of hosts.  First, on host one, run pingall as follows:

----
mininet> pingall
*** Ping: testing ping reachability
h35_2 -> h35_3 h36_2 h36_3
h35_3 -> h35_2 h36_2 h36_3
h36_2 -> h35_2 h35_3 h36_3
h36_3 -> h35_2 h35_3 h36_2
*** Results: 0% dropped (12/12 received)
----

and the same on host 2:

----
mininet> pingall
*** Ping: testing ping reachability
h35_4 -> h35_5 h36_4 h36_5
h35_5 -> h35_4 h36_4 h36_5
h36_4 -> h35_4 h35_5 h36_5
h36_5 -> h35_4 h35_5 h36_4
----

The hosts +h35_[n]+ are in EG1, in the subnet 10.0.35.1/24. Hosts +h36_[n]+ are in EG2, in the subnet 10.0.36.1/24.  These two tests therefore shows broadcast within the flood domain working to enable ARP, bridging within the endpoint group, and the functioning of the virtual router which is routing traffic between the two subnets.  It also shows the ICMP policy allowing the ping between the two groups.

Now we can test connectivity over the tunnel:

----
mininet> h35_2 ping -c1 10.0.35.4
PING 10.0.35.4 (10.0.35.4) 56(84) bytes of data.
64 bytes from 10.0.35.4: icmp_seq=1 ttl=64 time=1.78 ms

--- 10.0.35.4 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 1.786/1.786/1.786/0.000 ms
mininet> h35_2 ping -c1 10.0.35.5
PING 10.0.35.5 (10.0.35.5) 56(84) bytes of data.
64 bytes from 10.0.35.5: icmp_seq=1 ttl=64 time=2.59 ms

--- 10.0.35.5 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 2.597/2.597/2.597/0.000 ms
mininet> h35_2 ping -c1 10.0.36.4
PING 10.0.36.4 (10.0.36.4) 56(84) bytes of data.
64 bytes from 10.0.36.4: icmp_seq=1 ttl=62 time=2.64 ms

--- 10.0.36.4 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 2.641/2.641/2.641/0.000 ms
mininet> h35_2 ping -c1 10.0.36.5
PING 10.0.36.5 (10.0.36.5) 56(84) bytes of data.
64 bytes from 10.0.36.5: icmp_seq=1 ttl=62 time=2.93 ms

--- 10.0.36.5 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 2.936/2.936/2.936/0.000 ms
----

This shows all those same features working transparently across the tunnel to the hosts on the other switch.

===== HTTP

We expect HTTP to work only when going from EG1 to EG2, and only on port 80.  Let's check.  First, we'll start a web server on +h36_2+ by running this on host 1:

----
 mininet> h36_2 python -m SimpleHTTPServer 80
----

Note that this will block your prompt until you Ctrl-C it later.

Now on host 2, run:

----
mininet> h35_4 curl http://10.0.36.2
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   488  100   488    0     0  72944      0 --:--:-- --:--:-- --:--:-- 97600
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN"><html>
<title>Directory listing for /</title>
<body>
<h2>Directory listing for /</h2>
<hr>
<ul>
<li><a href="config.py">config.py</a>
<li><a href="config.pyc">config.pyc</a>
<li><a href="mininet_gbp.py">mininet_gbp.py</a>
<li><a href="mininet_gbp.pyc">mininet_gbp.pyc</a>
<li><a href="odl_gbp.py">odl_gbp.py</a>
<li><a href="odl_gbp.pyc">odl_gbp.pyc</a>
<li><a href="testOfOverlay.py">testOfOverlay.py</a>
</ul>
<hr>
</body>
</html>
----

You can see that the host in endpoint group 1 is able to access the server in endpoint group 2.

Let's try the reverse.  Ctrl-C the server on host 1 and then run:

----
 mininet> h35_2 python -m SimpleHTTPServer 80
----

We can still access the server from +h35_4+ on host 2, because it's in the same endpoint group:

----
mininet> h35_4 curl http://10.0.35.2
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   488  100   488    0     0  55625      0 --:--:-- --:--:-- --:--:-- 61000
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN"><html>
<title>Directory listing for /</title>
<body>
<h2>Directory listing for /</h2>
<hr>
<ul>
<li><a href="config.py">config.py</a>
<li><a href="config.pyc">config.pyc</a>
<li><a href="mininet_gbp.py">mininet_gbp.py</a>
<li><a href="mininet_gbp.pyc">mininet_gbp.pyc</a>
<li><a href="odl_gbp.py">odl_gbp.py</a>
<li><a href="odl_gbp.pyc">odl_gbp.pyc</a>
<li><a href="testOfOverlay.py">testOfOverlay.py</a>
</ul>
<hr>
</body>
</html>
----

But we cannot access it from +h36_4+ on host 2, because it's in a different endpoint group and our contract allows HTTP only in the other direction:

----
mininet> h36_4 curl http://10.0.35.2 --connect-timeout 3
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0
curl: (28) Connection timed out after 3001 milliseconds
----

=== Contact Information

Mailing List::
groupbasedpolicy-users@lists.opendaylight.org
IRC::
freenode.net #opendaylight-group-policy
Repository::
 https://git.opendaylight.org/gerrit/groupbasedpolicy

