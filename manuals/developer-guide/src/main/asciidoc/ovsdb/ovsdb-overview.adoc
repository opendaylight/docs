=== OVSDB Integration
The Open vSwitch database (OVSDB) Southbound Plugin component for OpenDaylight implements
the OVSDB  https://tools.ietf.org/html/rfc7047[RFC 7047] management protocol
that allows the southbound configuration of switches that support OVSDB. The
component comprises a library and a plugin. The OVSDB protocol
uses JSON-RPC calls to manipulate a physical or virtual switch that supports OVSDB.
Many vendors support OVSDB on various hardware platforms.
The OpenDaylight controller uses the library project to interact with an OVS
instance.

NOTE:
Read the OVSDB User Guide before you begin development.

==== OpenDaylight OVSDB integration
The OpenStack integration architecture uses the following technologies:

* https://tools.ietf.org/html/rfc7047[RFC 7047] - The Open vSwitch Database Management Protocol
* http://www.opennetworking.org/images/stories/downloads/sdn-resources/onf-specifications/openflow/openflow-switch-v1.3.4.pdf[OpenFlow v1.3]
* https://wiki.openstack.org/wiki/Neutron/ML2[OpenStack Neutron ML2 Plugin]

===== OpenDaylight Mechanism Driver for Openstack Neutron ML2
This code is a part of OpenStack and is available at: https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/drivers/mechanism_odl.py

The ODL neutron driver implementation can be found at: https://github.com/openstack/networking-odl

To make changes to this code, please read about https://wiki.openstack.org/wiki/NeutronDevelopment[Neutron Development].

Before submitting the code, run the following tests:

----
tox -e py27
tox -e pep8
----

===== Importing the code in to Eclipse or IntelliJ
To import code, look at either of the following pages:

* https://wiki.opendaylight.org/view/Eclipse_Setup[Getting started with Eclipse]
* https://wiki.opendaylight.org/view/OpenDaylight_Controller:Developing_With_Intellij[Developing with Intellij]

.Avoid conflicting project names
image::OVSDB_Eclipse.png[]

* To ensure that a project in Eclipse does not have a conflicting name in the workspace, select Advanced > Name Template > [groupId].[artifactId] when importing the project.

===== Browsing the code
The code is mirrored to https://github.com/opendaylight/ovsdb[GitHub] to make reading code online easier. 

===== Source code organization

The OVSDB project generates the following Karaf modules:

* ovsdb.karaf  -- all openstack netvirt related artifacts
* ovsdb.library-karaf -- the OVSDB library reference implementation
* ovsdb.openstack.net-virt-sfc-karaf  -- openflow service function chaining
* ovsdb.hwvtepsouthbound-karaf -- the hw_vtep schema southbound plugin
* ovsdb.southbound-karaf - the Open_vSwitch schema plugin

Following are a brief descriptions on directories you will find a the root ovsdb/ directory:

* _commons_ contains the parent POM file for Maven project which is used to get consistency of settings across the project.

* _features_ contains all the Karaf related feature files.

* _hwvtepsouthbound_ contains the hw_vtep southbound plugin.

* _karaf_ contains the ovsdb library and southbound and OpenStack bundles for the OpenStack integration.

* _library_ contains a schema-independent library that is a reference implementation for RFC 7047.

* _openstack_ contains the northbound handlers for Neutron used by OVSDB, as well as their providers. The NetVirt SFC implementation is also located here.

* _ovsdb-ui_ contains the DLUX implementation for displaying network virtualization.

* _resources_ contains useful scripts, how-tos, demos and other resources.

* _schemas_ contains the OVSDB schemas that are implemented in OpenDaylight.

* _southbound_ contains the plugin for converting from the OVSDB protocol to MD-SAL and vice-versa.

* _utils_ contains a collection of utilities for using the OpenFlow plugin, southbound, Neutron and other helper methods.

==== Building and running OVSDB
*Prerequisites* +

* JDK 1.7+
* Maven 3+

[[ovsdbBuildSteps]]
===== Building a Karaf feature and deploying it in an Opendaylight Karaf distribution +
. From the root ovsdb/ directory, run *mvn clean install*.
. Unzip the karaf-<VERSION_NUMBER>-SNAPSHOT.zip file created from step 1 in the directory ovsdb/karaf/target/:
----
unzip karaf-<VERSION_NUMBER>-SNAPSHOT.zip
----
===== Downloading OVSDB's Karaf distribution +
Instead of building, you can download the latest OVSDB distribution from the Nexus server. The link for that is:
----
https://nexus.opendaylight.org/content/repositories/opendaylight.snapshot/org/opendaylight/ovsdb/karaf/1.3.0-SNAPSHOT/
----

===== Running Karaf feature from OVSDB's Karaf distribution +

[[ovsdbStartingOdl]]
. Start ODL, from the unzipped directory
----
bin/karaf
----
. Once karaf has started, and you see the Opendaylight ascii art in the console, the last step is to start the OVSDB plugin framework with the following command in the karaf console: 
----
feature:install odl-ovsdb-openstack
----

====== Sample output from the Karaf console
----
opendaylight-user@root>feature:list | grep -i ovsdb 
opendaylight-user@root>feature:list -i | grep ovsdb
odl-ovsdb-southbound-api          | 1.2.1-SNAPSHOT   | x         | odl-ovsdb-southbound-1.2.1-SNAPSHOT     | OpenDaylight :: southbound :: api
odl-ovsdb-southbound-impl         | 1.2.1-SNAPSHOT   | x         | odl-ovsdb-southbound-1.2.1-SNAPSHOT     | OpenDaylight :: southbound :: impl
odl-ovsdb-southbound-impl-rest    | 1.2.1-SNAPSHOT   | x         | odl-ovsdb-southbound-1.2.1-SNAPSHOT     | OpenDaylight :: southbound :: impl :: REST
odl-ovsdb-southbound-impl-ui      | 1.2.1-SNAPSHOT   | x         | odl-ovsdb-southbound-1.2.1-SNAPSHOT     | OpenDaylight :: southbound :: impl :: UI
odl-ovsdb-library                 | 1.2.1-SNAPSHOT   | x         | odl-ovsdb-library-1.2.1-SNAPSHOT        | OpenDaylight :: library
odl-ovsdb-openstack               | 1.2.1-SNAPSHOT   | x         | ovsdb-1.2.1-SNAPSHOT                    | OpenDaylight :: OVSDB :: OpenStack Network Virtual
----

===== Testing patches
It is recommended that you test your patches locally before submission.

===== Neutron integration
To test patches to the Neutron integration, you need a http://devstack.org/guides/multinode-lab.html[Multi-Node Devstack Setup]. The ``resources`` folder contains sample ``local.conf`` files.

===== Open vSwitch
To test patches to the library, you will need a working http://openvswitch.org/[Open vSwitch]. Packages are available for most Linux distributions. If you would like to run multiple versions of Open vSwitch for testing you can use https://github.com/dave-tucker/docker-ovs[docker-ovs] to run Open vSwitch in https://www.docker.com/[Docker] containers. 

===== Mininet
http://mininet.org/[Mininet] is another useful resource for testing patches. Mininet creates multiple Open vSwitches connected in a configurable topology. 

===== Vagrant
The Vagrant file in the root of the OVSDB source code provides an easy way to create VMs for tests.

* To install Vagrant on your machine, follow the steps at: https://docs.vagrantup.com/v2/installation/[Installing Vagrant].

*Testing with Devstack*

. Start the controller.
----
vagrant up devstack-control
vagrant ssh devstack-control
cd devstack
./stack.sh
----
[start=2]
. Run the following:
----
vagrant up devstack-compute-1
vagrant ssh devstack-compute-1
cd devstack
./stack.sh
----
[start=3]
. To start testing, create a new VM.
----
nova boot --flavor m1.tiny --image $(nova image-list | grep 'cirros-0.3.1-x86_64-uec\s' | awk '{print $2}') --nic net-id=$(neutron net-list | grep private | awk '{print $2}') test
----
To create three, use the following:
----
nova boot --flavor m1.tiny --image $(nova image-list | grep 'cirros-0.3.1-x86_64-uec\s' | awk '{print $2}') --nic net-id=$(neutron net-list | grep private | awk '{print $2}') --num-instances 3 test
----
[start=4]
.To get a mininet installation for testing:
----
vagrant up mininet
vagrant ssh mininet
----
[start=5]
. Use the following to clean up when finished:
----
vagrant destroy
----

==== OVSDB integration design
===== Resources
See the following: +

* http://networkheresy.com/2012/09/15/remembering-the-management-plane/[Network Heresy]

See the OVSDB YouTube Channel for getting started videos and other tutorials: +

* http://www.youtube.com/channel/UCMYntfZ255XGgYFrxCNcAzA[ODL OVSDB Youtube Channel]
* https://wiki.opendaylight.org/view/OVSDB_Integration:Mininet_OVSDB_Tutorial[Mininet OVSDB Tutorial]
* https://wiki.opendaylight.org/view/OVSDB_Integration:Main#Getting_Started_with_OpenDaylight_OVSDB_Plugin_Network_Virtualization[OVSDB Getting Started]

==== OpenDaylight OVSDB southbound plugin architecture and design
OpenVSwitch (OVS) is generally accepted as the unofficial standard for Virtual Switching in the Open hypervisor based solutions. Every other Virtual Switch implementation, properietery or otherwise, uses OVS in some form.
For information on OVS, see http://openvswitch.org/[Open vSwitch].

In Software Defined Networking (SDN), controllers and applications interact using two channels: OpenFlow and OVSDB. OpenFlow addresses the forwarding-side of the OVS functionality. OVSDB, on the other hand, addresses the management-plane. 
A simple and concise overview of Open Virtual Switch Database(OVSDB) is available at: http://networkstatic.net/getting-started-ovsdb/

===== Overview of OpenDaylight Controller architecture
The OpenDaylight controller platform is designed as a highly modular and plugin based middleware that serves various network applications in a variety of use-cases. The modularity is achieved through the Java OSGi framework. The controller consists of many Java OSGi bundles that work together to provide the required
 controller functionalities. 
 
The bundles can be placed in the following broad categories: +

* Network Service Functional Modules (Examples: Topology Manager, Inventory Manager, Forwarding Rules Manager,and others) 
* NorthBound API Modules (Examples: Topology APIs, Bridge Domain APIs, Neutron APIs, Connection Manager APIs, and others) 
* Service Abstraction Layer(SAL)- (Inventory Services, DataPath Services, Topology Services, Network Config, and others) 
* SouthBound Plugins (OpenFlow Plugin, OVSDB Plugin, OpenDove Plugin, and others) 
* Application Modules (Simple Forwarding, Load Balancer)

Each layer of the Controller architecture performs specified tasks, and hence aids in modularity. 
While the Northbound API layer addresses all the REST-Based application needs, the SAL layer takes care of abstracting the SouthBound plugin protocol specifics from the Network Service functions. 
 
Each of the SouthBound Plugins serves a different purpose, with some overlapping.
For example, the OpenFlow plugin might serve the Data-Plane needs of an OVS element, while the OVSDB plugin can serve the management plane needs of the same OVS element.
As the Openflow Plugin talks OpenFlow protocol with the OVS element, the OVSDB plugin will use OVSDB schema over JSON-RPC transport.

==== OVSDB southbound plugin
The http://tools.ietf.org/html/draft-pfaff-ovsdb-proto-02[Open vSwitch Database Management Protocol-draft-02] and http://openvswitch.org/ovs-vswitchd.conf.db.5.pdf[Open vSwitch Manual] provide theoretical information about OVSDB.
The OVSDB protocol draft is generic enough to lay the groundwork on Wire Protocol and Database Operations, and the OVS Manual currently covers 13 tables leaving space for future OVS expansion, and vendor expansions on proprietary implementations.
The OVSDB Protocol is a database records transport protocol using JSON RPC1.0. For information on the protocol structure, see http://networkstatic.net/getting-started-ovsdb/[Getting Started with OVSDB].
The OpenDaylight OVSDB southbound plugin consists of one or more OSGi bundles addressing the following services or functionalities: +

* Connection Service - Based on Netty 
* Network Configuration Service 
* Bidirectional JSON-RPC Library 
* OVSDB Schema definitions and Object mappers 
* Overlay Tunnel management 
* OVSDB to OpenFlow plugin mapping service 
* Inventory Service 

==== Connection service
One of the primary services that most southbound plugins provide in Opendaylight a Connection Service. The service provides protocol specific connectivity to network elements, and supports the connectivity management services as specified by the OpenDaylight Connection Manager.
The connectivity services include: +

* Connection to a specified element given IP-address, L4-port, and other connectivity options (such as authentication,...) 
* Disconnection from an element 
* Handling Cluster Mode change notifications to support the OpenDaylight Clustering/High-Availability feature 

==== Network Configuration Service
The goal of the OpenDaylight Network Configuration services is to provide complete management plane solutions needed to successfully install, configure, and deploy the various SDN based network services. These are generic services which can be implemented in part or full by any south-bound protocol plugin.
The south-bound plugins can be either of the following: +

* The new network virtualization protocol plugins such as OVSDB JSON-RPC
* The traditional management protocols such as SNMP or any others in the middle. 

The above definition, and more information on Network Configuration Services, is available at : https://wiki.opendaylight.org/view/OpenDaylight_Controller:NetworkConfigurationServices 

===== Bidirectional JSON-RPC library
The OVSDB plugin implements a Bidirectional JSON-RPC library.  It is easy to design the library as a module that manages the Netty connection towards the Element. 

The main responsibilities of this Library are: +

* Demarshal and marshal JSON Strings to JSON objects 
* Demarshal and marshal JSON Strings from and to the Network Element.

===== OVSDB Schema definitions and Object mappers
The OVSDB Schema definitions and Object Mapping layer sits above the JSON-RPC library. It maps the generic JSON objects to OVSDB schema POJOs (Plain Old Java Object) and vice-versa. This layer mostly provides the Java Object definition for the corresponding OVSDB schema (13 of them) and also will provide much more friendly API abstractions on top of these object data. This helps in hiding the JSON semantics from the functional modules such as Configuration Service and Tunnel management.

On the demarshaling side the mapping logic differentiates the Request and Response messages as follows : +

* Request messages are mapped by its "method" 
* Response messages are mapped by their IDs which were originally populated by the Request message.
The JSON semantics of these OVSDB schema is quite complex.
The following figures summarize two of the end-to-end scenarios: +

.End-to-end handling of a Create Bridge request
image::ConfigurationService-example1.png[width=500]

.End-to-end handling of a monitor response
image::MonitorResponse.png[width=500]

===== Overlay tunnel management

Network Virtualization using OVS is achieved through Overlay Tunnels. The actual Type of the Tunnel may be GRE, VXLAN, or STT. The differences in the encapsulation and configuration decide the tunnel types. Establishing a tunnel using configuration service requires just the sending of OVSDB messages towards the ovsdb-server. However, the scaling issues that would arise on the state management at the data-plane (using OpenFlow) can get challenging. Also, this module can assist in various optimizations in the presence of Gateways. It can also help in providing Service guarantees for the VMs using these overlays with the help of underlay orchestration. 

===== OVSDB to OpenFlow plugin mapping service
The connect() of the ConnectionService  would result in a Node that represents an ovsdb-server. The CreateBridgeDomain() Configuration on the above Node would result in creating an OVS bridge. This OVS Bridge is an OpenFlow Agent for the OpenDaylight OpenFlow plugin with its own Node represented as (example) OF|xxxx.yyyy.zzzz. 
Without any help from the OVSDB plugin, the Node Mapping Service of the Controller platform would not be able to map the following: +
----
{OVSDB_NODE + BRIDGE_IDENTFIER} <---> {OF_NODE}.
----
Without such mapping, it would be extremely difficult for the applications to manage and maintain such nodes. This Mapping Service provided by the OVSDB plugin would essentially help in providing more value added services to the orchestration layers that sit atop the Northbound APIs (such as OpenStack). 

==== OpenDaylight OVSDB Developer Getting Started Video Series
The video series were started to help developers bootstrap into OVSDB development.

* http://www.youtube.com/watch?v=ieB645oCIPs[OpenDaylight OVSDB Developer Getting Started]
* http://www.youtube.com/watch?v=xgevyaQ12cg[OpenDaylight OVSDB Developer Getting Started - Northbound API Usage]
* http://www.youtube.com/watch?v=xgevyaQ12cg[OpenDaylight OVSDB Developer Getting Started - Java APIs]
* http://www.youtube.com/watch?v=NayuY6J-AMA[OpenDaylight OVSDB Developer Getting Started - OpenStack Integration OpenFlow v1.0]

===== Other developer tutorials

* https://docs.google.com/presentation/d/1KIuNDuUJGGEV37Zk9yzx9OSnWExt4iD2Z7afycFLf_I/edit?usp=sharing[OVSDB NetVirt Tutorial]
* https://www.youtube.com/watch?v=2axNKHvt5MY&list=PL8F5jrwEpGAiJG252ShQudYeodGSsks2l&index=43[Youtube of OVSDB NetVirt tutorial]
* https://wiki.opendaylight.org/view/OVSDB:OVSDB_OpenStack_Guide[OVSDB OpenFlow v1.3 Neutron ML2 Integration]
* http://networkstatic.net/getting-started-ovsdb/[Open vSwitch Database Table Explanations and Simple Jackson Tutorial]

==== OVSDB integration: New features
===== Schema independent library
The OVS connection is a node which can have multiple databases. Each database is represented by a schema. A single connection can have multiple schemas.
OSVDB supports multiple schemas. Currently, these are two schemas available in the
OVSDB, but there is no restriction on the number of schemas. Owing to the Northbound v3 API, no code changes in ODL are needed for supporting additional schemas.

Schemas: +

*  openvswitch : Schema wrapper that represents http://openvswitch.org/ovs-vswitchd.conf.db.5.pdf
*  hardwarevtep: Schema wrapper that represents http://openvswitch.org/docs/vtep.5.pdf

===== Port security
Based on the fact that security rules can be obtained from a port object, OVSDB can apply Open Flow rules. These rules will match on what types of traffic the Openstack tenant VM is allowed to use.
 
Support for security groups is very experimental. There are limitations in determining the state of flows in the Open vSwitch. See http://%20https//www.youtube.com/watch?v=DSop2uLJZS8[Open vSwitch and the Intelligent Edge] from Justin Petit for a deep dive into the challenges we faced creating a flow based port security implementation. The current set of rules that will be installed only supports filtering of the TCP protocol. This is because via a Nicira TCP_Flag read we can match on a flows TCP_SYN flag, and permit or deny the flow based on the Neutron port security rules. If rules are requested for ICMP and UDP, they are ignored until greater visibility from the Linux kernel is available as outlined in the OpenStack presentation mentioned earlier. 

Using the port security groups of Neutron, one can add rules that restrict the network access of the tenants. The OVSDB Neutron integration checks the port security rules configured, and apply them by means of openflow rules. 

Through the ML2 interface, Neutron security rules are available in the port object, following this scope: Neutron Port -> Security Group -> Security Rules. 

The current rules are applied on the basis of the following attributes: ingress/egress, tcp protocol, port range, and prefix.
 
====== OpenStack workflow
. Create a stack.
. Add the network and subnet. 
. Add the Security Group and Rules.

NOTE: This is no different than what users normally do in regular openstack deployments. 
----
neutron security-group-create group1 --description "Group 1"
neutron security-group-list
neutron security-group-rule-create --direction ingress --protocol tcp group1
----
[start=4]
. Start the tenant, specifying the security-group.
----
nova boot --flavor m1.tiny \
--image $(nova image-list | grep 'cirros-0.3.1-x86_64-uec\s' | awk '{print $2}') \
--nic net-id=$(neutron net-list | grep 'vxlan2' | awk '{print $2}') vxlan2 \
--security-groups group1
----
====== Examples: Rules supported
----
neutron security-group-create group2 --description "Group 2"
neutron security-group-rule-create --direction ingress --protocol tcp --port-range-min 54 group2
neutron security-group-rule-create --direction ingress --protocol tcp --port-range-min 80 group2
neutron security-group-rule-create --direction ingress --protocol tcp --port-range-min 1633 group2
neutron security-group-rule-create --direction ingress --protocol tcp --port-range-min 22 group2
----
----
neutron security-group-create group3 --description "Group 3"
neutron security-group-rule-create --direction ingress --protocol tcp --remote-ip-prefix 10.200.0.0/16 group3
----
----
neutron security-group-create group4 --description "Group 4"
neutron security-group-rule-create --direction ingress --remote-ip-prefix 172.24.0.0/16 group4
----
----
neutron security-group-create group5 --description "Group 5"
neutron security-group-rule-create --direction ingress --protocol tcp group5
neutron security-group-rule-create --direction ingress --protocol tcp --port-range-min 54 group5
neutron security-group-rule-create --direction ingress --protocol tcp --port-range-min 80 group5
neutron security-group-rule-create --direction ingress --protocol tcp --port-range-min 1633 group5
neutron security-group-rule-create --direction ingress --protocol tcp --port-range-min 22 group5
----
----
neutron security-group-create group6 --description "Group 6"
neutron security-group-rule-create --direction ingress --protocol tcp --remote-ip-prefix 0.0.0.0/0 group6
----
----
neutron security-group-create group7 --description "Group 7"
neutron security-group-rule-create --direction egress --protocol tcp --port-range-min 443 --remote-ip-prefix 172.16.240.128/25 group7
----
*Reference gist*:https://gist.github.com/anonymous/1543a410d57f491352c8[Gist]

====== Security group rules supported in ODL 
The following rules formata are supported in the current implementation. The direction (ingress/egress) is always expected. Rules are implemented such that tcp-syn packets that do not satisfy the rules are dropped.
[cols="3", width="60%"]
|===
| Proto | Port | IP Prefix

|TCP |x |x
|Any | Any |x
|TCP |x |Any
|TCP |Any |Any
|===

====== Limitations
* Soon, conntrack will be supported by OVS. Until then, TCP flags are used as way of checking for connection state. Specifically, that is done by matching on the TCP-SYN flag.
* The param '--port-range-max' in 'security-group-rule-create' is not used until the implementation uses contrack. 
* No UDP/ICMP specific match support is provided.
* No IPv6 support is provided.

===== L3 forwarding
OVSDB extends support for the usage of an ODL-Neutron-driver so that OVSDB can configure OF 1.3 rules to route IPv4 packets. The driver eliminates the need for the router of the L3 Agent. In order to accomplish that, OVS 2.1 or a newer version is required.
OVSDB also supports inbound/outbound NAT, floating IPs.

====== Starting OVSDB and OpenStack
. Build or download OVSDB distribution, as mentioned in <<ovsdbBuildSteps,building a Karaf feature section>>.
. http://docs.vagrantup.com/v2/installation/index.html[Install Vagrant].

[start=3]
. Enable the L3 Forwarding feature:
----
echo 'ovsdb.l3.fwd.enabled=yes' >> ./opendaylight/configuration/config.ini
echo 'ovsdb.l3gateway.mac=${GATEWAY_MAC}' >> ./configuration/config.ini
----
[start=4]
. Run the following commands to get the odl neutron drivers:
[start=5]
----
git clone https://github.com/dave-tucker/odl-neutron-drivers.git
cd odl-neutron-drivers
vagrant up devstack-control devstack-compute-1
----
[start=6]
. Use ssh to go to the control node, and clone odl-neutron-drivers again:
----
vagrant ssh devstack-control
git clone https://github.com/dave-tucker/odl-neutron-drivers.git
cd odl-neutron-drivers
sudo python setup.py install
*leave this shell open*
----
[start=7]
. Start odl, as mentioned in <<ovsdbStartingOdl,running Karaf feature section>>.
[start=8]
. To see processing of neutron event related to L3, do this from prompt:
----
log:set debug org.opendaylight.ovsdb.openstack.netvirt.impl.NeutronL3Adapter
----
[start=9]
. From shell, do one of the following: open on ssh into control node or vagrant ssh devstack-control.
----
cd ~/devstack && ./stack.sh
----
[start=10]
. From a new shell in the host system, run the following:
----
cd odl-neutron-drivers
vagrant ssh devstack-compute-1
cd ~/devstack && ./stack.sh
----

====== OpenStack workflow
.Sample workflow
image::L3FwdSample.png[height=250]

Use the following steps to set up a workflow like the one shown in figure above.

. Set up authentication. From shell on stack control or vagrant ssh devstack-control:
----
source openrc admin admin
----

----
rm -f id_rsa_demo* ; ssh-keygen -t rsa -b 2048 -N  -f id_rsa_demo
 nova keypair-add --pub-key  id_rsa_demo.pub  demo_key
 # nova keypair-list
----
[start=2]
. Create two networks and two subnets.
----
neutron net-create net1 --tenant-id $(keystone tenant-list | grep '\s'admin | awk '{print $2}') \
 --provider:network_type gre --provider:segmentation_id 555
----
----
neutron subnet-create --tenant-id $(keystone tenant-list | grep '\s'admin | awk '{print $2}') \
net1 10.0.0.0/16 --name subnet1 --dns-nameserver 8.8.8.8
----
----
neutron net-create net2 --tenant-id $(keystone tenant-list | grep '\s'admin | awk '{print $2}') \
 --provider:network_type gre --provider:segmentation_id 556
----
----
neutron subnet-create --tenant-id $(keystone tenant-list | grep '\s'admin | awk '{print $2}') \
 net2 20.0.0.0/16 --name subnet2 --dns-nameserver 8.8.8.8
----
[start=3]
. Create a router, and add an interface to each of the two subnets.
----
neutron router-create demorouter --tenant-id $(keystone tenant-list | grep '\s'admin | awk '{print $2}')
 neutron router-interface-add demorouter subnet1
 neutron router-interface-add demorouter subnet2
 # neutron router-port-list demorouter
----
[start=4]
. Create two tenant instances.
----
nova boot --poll --flavor m1.nano --image $(nova image-list | grep 'cirros-0.3.2-x86_64-uec\s' | awk '{print $2}') \
 --nic net-id=$(neutron net-list | grep -w net1 | awk '{print $2}'),v4-fixed-ip=10.0.0.10 \
 --availability-zone nova:devstack-control \
 --key-name demo_key host10
----
----
nova boot --poll --flavor m1.nano --image $(nova image-list | grep 'cirros-0.3.2-x86_64-uec\s' | awk '{print $2}') \
 --nic net-id=$(neutron net-list | grep -w net2 | awk '{print $2}'),v4-fixed-ip=20.0.0.20 \
 --availability-zone nova:devstack-compute-1 \
 --key-name demo_key host20
----

====== Limitations
* To use this feature, you need OVS 2.1 or newer version.
* Owing to OF limitations, icmp responses due to routing failures, like ttl expired or host unreacheable, are not generated.
* The MAC address of the default route is not automatically mapped. In order to route to L3 destinations outside the networks of the tenant, the manual configuration of the default route is necessary. To provide the MAC address of the default route, use ovsdb.l3gateway.mac in file configuration/config.ini ; 
* This feature is Tech preview, which depends on later versions of OpenStack to be used without the provided neutron-driver. 
* No IPv6 support is provided.
 
*More information on L3 forwarding*: +

* odl-neutron-driver: https://github.com/dave-tucker/odl-neutron-drivers
* OF rules example: http://dtucker.co.uk/hack/building-a-router-with-openvswitch.html

===== LBaaS
Load-Balancing-as-a-Service (LBaaS) creates an Open vSwitch powered L3-L4 stateless load-balancer in a virtualized network environment so that individual TCP connections destined to a designated virtual IP (VIP) are sent to the appropriate servers (that is to say, serving app VMs). The load-balancer works in a session-preserving, proactive manner without involving the controller during flow setup.

A Neutron northbound interface is provided to create a VIP which will map to a pool of servers (that is to say, members) within a subnet. The pools consist of members identified by an IP address. The goal is to closely match the API to the OpenStack LBaaS v2 API: http://docs.openstack.org/api/openstack-network/2.0/content/lbaas_ext.html.

====== Creating an OpenStack workflow
. Create a subnet. 
. Create a floating VIP 'A' that maps to a private VIP 'B'. 
. Create a Loadbalancer pool 'X'. 
----
neutron lb-pool-create --name http-pool --lb-method ROUND_ROBIN --protocol HTTP --subnet-id XYZ
----
[start=4]
. Create a Loadbalancer pool member 'Y' and associate with pool 'X'. 
----
neutron lb-member-create --address 10.0.0.10 --protocol-port 80 http-pool
neutron lb-member-create --address 10.0.0.11 --protocol-port 80 http-pool
neutron lb-member-create --address 10.0.0.12 --protocol-port 80 http-pool
neutron lb-member-create --address 10.0.0.13 --protocol-port 80 http-pool
----
[start=5]
. Create a Loadbalancer instance 'Z', and associate pool 'X' and VIP 'B' with it.
----
neutron lb-vip-create --name http-vip --protocol-port 80 --protocol HTTP --subnet-id XYZ http-pool
----

====== Implementation

The current implementation of the proactive stateless load-balancer was made using "multipath" action in the Open vSwitch. The "multipath" action takes a max_link parameter value (which is same as the number of pool members) as input, and performs a hash of the fields to get a value between (0, max_link). The value of the hash is used as an index to select a pool member to handle that session. 

===== Open vSwitch rules
Assuming that table=20 contains all the rules to forward the traffic destined for a specific destination MAC address, the following are the rules needed to be programmed in the LBaaS service table=10. The programmed rules makes the translation from the VIP to a different pool member for every session.

* Proactive forward rules:
----
sudo ovs-ofctl -O OpenFlow13 add-flow s1 "table=10,reg0=0,ip,nw_dst=10.0.0.5,actions=load:0x1->NXM_NX_REG0[[]],multipath(symmetric_l4, 1024, modulo_n, 4, 0, NXM_NX_REG1[0..12]),resubmit(,10)"
sudo ovs-ofctl -O OpenFlow13 add-flow s1 table=10,reg0=1,nw_dst=10.0.0.5,ip,reg1=0,actions=mod_dl_dst:00:00:00:00:00:10,mod_nw_dst:10.0.0.10,goto_table:20
sudo ovs-ofctl -O OpenFlow13 add-flow s1 table=10,reg0=1,nw_dst=10.0.0.5,ip,reg1=1,actions=mod_dl_dst:00:00:00:00:00:11,mod_nw_dst:10.0.0.11,goto_table:20
sudo ovs-ofctl -O OpenFlow13 add-flow s1 table=10,reg0=1,nw_dst=10.0.0.5,ip,reg1=2,actions=mod_dl_dst:00:00:00:00:00:12,mod_nw_dst:10.0.0.12,goto_table:20
sudo ovs-ofctl -O OpenFlow13 add-flow s1 table=10,reg0=1,nw_dst=10.0.0.5,ip,reg1=3,actions=mod_dl_dst:00:00:00:00:00:13,mod_nw_dst:10.0.0.13,goto_table:20
----
* Proactive reverse rules: 
----
sudo ovs-ofctl -O OpenFlow13 add-flow s1 table=10,ip,tcp,tp_src=80,actions=mod_dl_src:00:00:00:00:00:05,mod_nw_src:10.0.0.5,goto_table:20
---- 

====== OVSDB project code
The current implementation handles all neutron calls in the net-virt/LBaaSHandler.java code, and makes calls to the net-virt-providers/LoadBalancerService to program appropriate flowmods. The rules are updated whenever there is a change in the Neutron LBaaS settings. There is no cache of state kept in the net-virt or providers. 

====== Limitations
Owing to the inflexibility of the multipath action, the existing LBaaS implementation comes with some limitations: 

* TCP, HTTP or HTTPS are supported protocols for the pool. (Caution: You can lose access to the members if you assign {Proto:TCP, Port:22} to LB) 

* Member weights are ignored. 
* The update of an LB instance is done as a delete + add, and not an actual delta. 
* The update of an LB member is not supported (because weights are ignored). 
* Deletion of an LB member leads to the reprogramming of the LB on all nodes (because of the way multipath does link hash).
* There is only a single LB instance per subnet because the pool-id is not reported in the create load-balancer call. 









                       


